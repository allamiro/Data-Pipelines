# Setup One KAFKA Broker

![alt text](https://github.com/allamiro/Data-Pipelines/blob/master/KAFKA/kafka-onebroker.PNG)


### Install JAVA 
```
yum install wget tar  unzip java -y
java -version
```
### Download KAFKA

```
cd /tmp
wget https://downloads.apache.org/kafka/3.6.2/kafka_2.12-3.6.2.tgz
```

Extract the Kafka archive file 

```
tar -xzvf kafka_2.12-3.6.2.tgz
```


Extract the Kafka archive file 

```
mv kafka_2.12-3.6.2 /etc/kafka

```



# Setup kafka  Cluster 

![alt text](https://github.com/allamiro/Data-Pipelines/blob/master/KAFKA/kafka-cluster.PNG)


## On Each Kafka Server

### Install JAVA 

```
yum install wget tar unzip java -y
java -version
```

### Download Kafka and Zookeeper

On each server KFK1, KFK2 , KFK3 download Kafka and Zokeeper to the tmp directory 

```
cd /tmp
wget https://downloads.apache.org/kafka/3.6.2/kafka_2.12-3.6.2.tgz

```

On each server extract the Kafka archive file 

```

tar -xzvf kafka_2.12-3.6.2.tgz

```


### Configure Zookeeper


```
mv kafka_2.12-3.6.2 /etc/kafka

mkdir -p /var/lib/zookeeper/
```

Update Zookeeper properties file on every server

```
cat <<EOF | sudo tee /etc/kafka/config/zookeeper.properties
tickTime=2000
dataDir=/var/lib/zookeeper
clientPort=2181
initLimit=5
syncLimit=2
server.1=kfk1:2888:3888
server.2=kfk2:2888:3888
server.3=kfk3:2888:3888
EOF
```


Add the following Firewall rules on every server 

```
firewall-cmd --permanent --add-port=2181/tcp
firewall-cmd --add-port=2888-3888/tcp --permanent
firewall-cmd --reload
```
Create the broker id file on every kafka server

```
touch /var/lib/zookeeper/myid
```

### On KFK1
=======
```
cat <<EOF | sudo tee /var/lib/zookeeper/myid
1
EOF
```
### On KFK2
=======
```
cat <<EOF | sudo tee /var/lib/zookeeper/myid
2
EOF
```
### On KFK3
=======
```
cat <<EOF | sudo tee /var/lib/zookeeper/myid
3
EOF
```

Create a Zookeeper Service file on every server

```
cat <<EOF | sudo tee /etc/systemd/system/zoo.service
[Unit]
Description=Zookeeper Service
WantedBy=multi-user.target

[Service]
Type=simple
ExecStart=/etc/kafka/bin/zookeeper-server-start.sh /etc/kafka/config/zookeeper.properties
Restart=always
EOF
```


Start Zookeeper Service

```
systemctl daemon-reload
systemctl start zoo
systemctl status -l zoo
systemctl enable  zoo
```
### Configure KAFKA Brokers 


Update KAFKA properties file on every server

### On KFK1
=======

```
cat <<EOF | sudo tee /etc/kafka/config/server.properties
# Kafka server.properties configuration
# Template generated by Ansible

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=1

############################# Socket Server Settings #############################

# The address the socket server listens on.
listeners=PLAINTEXT://kafka1.example.com:9092

# Hostname and port the broker will advertise to producers and consumers.
#advertised.listeners=PLAINTEXT://kafka1.example.com.local.domain:9092

# Number of threads for network requests
num.network.threads=3

# Number of threads for I/O
num.io.threads=8

# Send buffer (SO_SNDBUF) size
socket.send.buffer.bytes=102400

# Receive buffer (SO_RCVBUF) size
socket.receive.buffer.bytes=102400

# Maximum size of request that the socket server will accept
socket.request.max.bytes=104857600

############################# Log Basics #############################

# Directories to store log files
log.dirs=/var/log/kafka-logs

# Number of log partitions per topic
num.partitions=3

# Number of threads per data directory for log recovery and flushing
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings  #############################

# Replication factors for internal topics
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

############################# Log Flush Policy #############################

# Interval at which to force a flush of data to disk
#log.flush.interval.messages=10000
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# Log retention hours
log.retention.hours=168

# Size-based log retention
#log.retention.bytes=1073741824

# Maximum size of a log segment file
log.segment.bytes=1073741824

# Check interval for log retention
log.retention.check.interval.ms=300000

############################# Zookeeper #############################

# Zookeeper connection string
zookeeper.connect=kafka1.example.com:2181,kafka2.example.com:2181,kafka3.example.com:2181
# Zookeeper connection timeout
zookeeper.connection.timeout.ms=6000

############################# Group Coordinator Settings #############################

# Initial rebalance delay
group.initial.rebalance.delay.ms=0


EOF
```



### On KFK2
=======

```
cat <<EOF | sudo tee /etc/kafka/config/server.properties

# Kafka server.properties configuration
# Template generated by Ansible

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=2

############################# Socket Server Settings #############################

# The address the socket server listens on.
listeners=PLAINTEXT://kafka2.example.com:9092

# Hostname and port the broker will advertise to producers and consumers.
#advertised.listeners=PLAINTEXT://kafka2.example.com.local.domain:9092

# Number of threads for network requests
num.network.threads=3

# Number of threads for I/O
num.io.threads=8

# Send buffer (SO_SNDBUF) size
socket.send.buffer.bytes=102400

# Receive buffer (SO_RCVBUF) size
socket.receive.buffer.bytes=102400

# Maximum size of request that the socket server will accept
socket.request.max.bytes=104857600

############################# Log Basics #############################

# Directories to store log files
log.dirs=/var/log/kafka-logs

# Number of log partitions per topic
num.partitions=3

# Number of threads per data directory for log recovery and flushing
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings  #############################

# Replication factors for internal topics
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

############################# Log Flush Policy #############################

# Interval at which to force a flush of data to disk
#log.flush.interval.messages=10000
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# Log retention hours
log.retention.hours=168

# Size-based log retention
#log.retention.bytes=1073741824

# Maximum size of a log segment file
log.segment.bytes=1073741824

# Check interval for log retention
log.retention.check.interval.ms=300000

############################# Zookeeper #############################

# Zookeeper connection string
zookeeper.connect=kafka1.example.com:2181,kafka2.example.com:2181,kafka3.example.com:2181
# Zookeeper connection timeout
zookeeper.connection.timeout.ms=6000

############################# Group Coordinator Settings #############################

# Initial rebalance delay
group.initial.rebalance.delay.ms=0

EOF
```


### On KFK3
=======

```
cat <<EOF | sudo tee /etc/kafka/config/server.properties
# Kafka server.properties configuration
# Template generated by Ansible

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=3

############################# Socket Server Settings #############################

# The address the socket server listens on.
listeners=PLAINTEXT://kafka3.example.com:9092

# Hostname and port the broker will advertise to producers and consumers.
#advertised.listeners=PLAINTEXT://kafka3.example.com.local.domain:9092

# Number of threads for network requests
num.network.threads=3

# Number of threads for I/O
num.io.threads=8

# Send buffer (SO_SNDBUF) size
socket.send.buffer.bytes=102400

# Receive buffer (SO_RCVBUF) size
socket.receive.buffer.bytes=102400

# Maximum size of request that the socket server will accept
socket.request.max.bytes=104857600

############################# Log Basics #############################

# Directories to store log files
log.dirs=/var/log/kafka-logs

# Number of log partitions per topic
num.partitions=3

# Number of threads per data directory for log recovery and flushing
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings  #############################

# Replication factors for internal topics
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

############################# Log Flush Policy #############################

# Interval at which to force a flush of data to disk
#log.flush.interval.messages=10000
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# Log retention hours
log.retention.hours=168

# Size-based log retention
#log.retention.bytes=1073741824

# Maximum size of a log segment file
log.segment.bytes=1073741824

# Check interval for log retention
log.retention.check.interval.ms=300000

############################# Zookeeper #############################

# Zookeeper connection string
zookeeper.connect=kafka1.example.com:2181,kafka2.example.com:2181,kafka3.example.com:2181
# Zookeeper connection timeout
zookeeper.connection.timeout.ms=6000

############################# Group Coordinator Settings #############################

# Initial rebalance delay
group.initial.rebalance.delay.ms=0


EOF
```


* Create a KAFKA Service file on every server


```
cat <<EOF | sudo tee /etc/systemd/system/kafka.service
[Unit]
Description=Kafka Service
WantedBy=multi-user.target

[Service]
Type=simple
ExecStart=/etc/kafka/bin/kafka-server-start.sh /etc/kafka/config/server.properties
Restart=always
EOF
```
* Add the following Firewall rules on every server 

```
firewall-cmd --permanent --add-port=9092/tcp
firewall-cmd --reload
```


Start KAFKA Service


```
systemctl daemon-reload
systemctl start kafka
systemctl status -l kafka
systemctl enable  kafka
```

Add KAFKA TOPICS  

The topics that I will be adding 
DB : For Database Server logs
WINDOWS : For Windows Logs
LINUX: FOR LINUX Rsyslog logs
NETWORK: For network devices logs ( Firewall , routers , etc )
APPS: For application logs
IOT : For internet of things devices logs



To Create KAFKA topic 

```
sh /etc/kafka/bin/kafka-topics.sh --create --bootstrap-server kafka1.example.com:9092 --replication-factor 2 --partitions 3 --topic  DB
sh /etc/kafka/bin/kafka-topics.sh --create --bootstrap-server kafka1.example.com:9092 --replication-factor 2 --partitions 3 --topic  IOT
sh /etc/kafka/bin/kafka-topics.sh --create --bootstrap-server kafka1.example.com:9092 --replication-factor 2 --partitions 3 --topic  LINUX
sh /etc/kafka/bin/kafka-topics.sh --create --bootstrap-server kafka1.example.com:9092 --replication-factor 2 --partitions 3 --topic WINDOWS
sh /etc/kafka/bin/kafka-topics.sh --create --bootstrap-server kafka1.example.com:9092 --replication-factor 2 --partitions 3 --topic NETWORK

```

To check the replication of your topics 

```
 sh /etc/kafka/bin/kafka-topics.sh  --describe --topic DB --bootstrap-server kafka1.example.com:9092 
Topic: DB	TopicId: r7zjkfxwQxmcXSx2o0s5_Q	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824
	Topic: DB	Partition: 0	Leader: 2	Replicas: 2,1	Isr: 2,1
	Topic: DB	Partition: 1	Leader: 3	Replicas: 3,2	Isr: 3,2
	Topic: DB	Partition: 2	Leader: 1	Replicas: 1,3	Isr: 1,3
[root@kafka1 ~]# sh /etc/kafka/bin/kafka-topics.sh  --describe --topic WINDOWS --bootstrap-server kafka1.example.com:9092 
Topic: WINDOWS	TopicId: kxPiuKD4RnKagBcmP1gxCg	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824
	Topic: WINDOWS	Partition: 0	Leader: 3	Replicas: 3,1	Isr: 3,1
	Topic: WINDOWS	Partition: 1	Leader: 1	Replicas: 1,2	Isr: 1,2
	Topic: WINDOWS	Partition: 2	Leader: 2	Replicas: 2,3	Isr: 2,3
[root@kafka1 ~]# sh /etc/kafka/bin/kafka-topics.sh  --describe --topic LINUX --bootstrap-server kafka1.example.com:9092 
Topic: LINUX	TopicId: jRYRP5EyTHeMGJZNYejguw	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824
	Topic: LINUX	Partition: 0	Leader: 3	Replicas: 3,1	Isr: 3,1
	Topic: LINUX	Partition: 1	Leader: 1	Replicas: 1,2	Isr: 1,2
	Topic: LINUX	Partition: 2	Leader: 2	Replicas: 2,3	Isr: 2,3
[root@kafka1 ~]# sh /etc/kafka/bin/kafka-topics.sh  --describe --topic IOT --bootstrap-server kafka1.example.com:9092 
Topic: IOT	TopicId: iNT2DcpfRdOoDRqNyAAjbQ	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824
	Topic: IOT	Partition: 0	Leader: 2	Replicas: 2,1	Isr: 2,1
	Topic: IOT	Partition: 1	Leader: 3	Replicas: 3,2	Isr: 3,2
	Topic: IOT	Partition: 2	Leader: 1	Replicas: 1,3	Isr: 1,3


```


```
sh /etc/kafka/bin/kafka-topics.sh  --describe --bootstrap-server kafka1.example.com:9092 
Topic: APPLICATIONS	TopicId: IqhV7b88R7WmsR896YM5Cw	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824
	Topic: APPLICATIONS	Partition: 0	Leader: 1	Replicas: 1,2	Isr: 1,2
	Topic: APPLICATIONS	Partition: 1	Leader: 2	Replicas: 2,3	Isr: 2,3
	Topic: APPLICATIONS	Partition: 2	Leader: 3	Replicas: 3,1	Isr: 3,1
Topic: WINDOWS	TopicId: kxPiuKD4RnKagBcmP1gxCg	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824
	Topic: WINDOWS	Partition: 0	Leader: 3	Replicas: 3,1	Isr: 3,1
	Topic: WINDOWS	Partition: 1	Leader: 1	Replicas: 1,2	Isr: 1,2
	Topic: WINDOWS	Partition: 2	Leader: 2	Replicas: 2,3	Isr: 2,3
Topic: NETWORK	TopicId: y0OehbWGS_OxKgccKKRKZg	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824
	Topic: NETWORK	Partition: 0	Leader: 1	Replicas: 1,3	Isr: 1,3
	Topic: NETWORK	Partition: 1	Leader: 2	Replicas: 2,1	Isr: 2,1
	Topic: NETWORK	Partition: 2	Leader: 3	Replicas: 3,2	Isr: 3,2
Topic: LINUX	TopicId: jRYRP5EyTHeMGJZNYejguw	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824
	Topic: LINUX	Partition: 0	Leader: 3	Replicas: 3,1	Isr: 3,1
	Topic: LINUX	Partition: 1	Leader: 1	Replicas: 1,2	Isr: 1,2
	Topic: LINUX	Partition: 2	Leader: 2	Replicas: 2,3	Isr: 2,3
Topic: DB	TopicId: r7zjkfxwQxmcXSx2o0s5_Q	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824
	Topic: DB	Partition: 0	Leader: 2	Replicas: 2,1	Isr: 2,1
	Topic: DB	Partition: 1	Leader: 3	Replicas: 3,2	Isr: 3,2
	Topic: DB	Partition: 2	Leader: 1	Replicas: 1,3	Isr: 1,3
Topic: IOT	TopicId: iNT2DcpfRdOoDRqNyAAjbQ	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824
	Topic: IOT	Partition: 0	Leader: 2	Replicas: 2,1	Isr: 2,1
	Topic: IOT	Partition: 1	Leader: 3	Replicas: 3,2	Isr: 3,2
	Topic: IOT	Partition: 2	Leader: 1	Replicas: 1,3	Isr: 1,3


```
Start Sending Data to KAFKA 

